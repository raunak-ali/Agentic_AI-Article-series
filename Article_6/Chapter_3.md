---

# ğŸ”¹ 3. Document Ingestion & Chunking

If Article 5 was about â€œmemory,â€ then this chapter is about **feeding your agents reliable knowledge**. In RAG (Retrieval-Augmented Generation), your LLM is only as good as the data pipeline behind it. That pipeline starts with **document ingestion and chunking**.

---

## ğŸ—ï¸ The Ingestion Pipeline

Hereâ€™s the high-level flow every RAG project follows:

```
[Raw Documents]  
      â”‚
      â–¼
[Preprocessing] â†’ clean text, normalize, remove PII  
      â”‚
      â–¼
[Chunking] â†’ split into 500â€“1,200 tokens w/ overlap  
      â”‚
      â–¼
[Attach Metadata] â†’ {source, page, timestamp}  
      â”‚
      â–¼
[Vector Database] (FAISS, Chroma, Pinecone)  
      â”‚
      â–¼
[Retriever] â†’ feeds context into LLM  
```

---

## ğŸ§¹ Step 1. Preprocessing

Before splitting, make the text **clean and uniform**.

* Remove headers/footers that repeat on every page.
* Normalize whitespace, Unicode symbols, weird line breaks.
* Strip sensitive data (PII) if your use case demands compliance.

**Tools:**

* `pdfplumber`, `pymupdf`, or `pypdf` â†’ for PDFs.
* `python-docx` â†’ for Word files.
* `BeautifulSoup4` â†’ for HTML.
* Custom scrapers â†’ for websites or databases.

---

## âœ‚ï¸ Step 2. Chunking

Chunking = splitting text into **bite-sized pieces** the LLM can handle.

* **Size:** 500â€“1,200 tokens is the sweet spot.

  * Too small = no context.
  * Too large = wasted tokens + poor retrieval.
* **Overlap:** add 50â€“200 tokens of overlap between chunks.

  * Prevents â€œcut-offâ€ mid-thought.
* **Boundaries:** split at natural paragraph/sentence breaks.

**Pseudo Code:**

```python
chunks = chunk_document(text, chunk_size=800, overlap=150)
for i, ch in enumerate(chunks):
    upsert_to_vector_db(
        id=f"{doc_id}_{i}", 
        text=ch, 
        metadata={"source": doc_name, "page": i}
    )
```

---

## âœ… Good vs âŒ Bad Chunking

| Aspect               | âŒ Bad Chunking Example                                | âœ… Good Chunking Example                                   |
| -------------------- | ----------------------------------------------------- | --------------------------------------------------------- |
| **Split Size**       | 50â€“100 tokens â†’ too small, loses context              | 800 tokens â†’ enough context for retrieval                 |
| **Overlap**          | 0 overlap â†’ broken thoughts mid-paragraph             | 150 token overlap â†’ smooth continuity                     |
| **Coherence**        | Splits mid-sentence (â€œEmployees are entitledâ€¦ / 18â€¦â€) | Splits at sentence or paragraph boundaries                |
| **Metadata**         | No metadata attached                                  | Metadata includes `{"source": "handbook.pdf", "page": 3}` |
| **Retrieval Output** | â€œ18â€ â†’ ambiguous, useless                             | â€œEmployees are entitled to 18 vacation days per yearâ€     |
| **Business Impact**  | Misleading answers, hallucinations                    | Clear, grounded answers users can trust                   |

ğŸ‘‰ Think of it like cutting a book into flashcards:

* Bad chunking = random sentences, shuffled order.
* Good chunking = coherent paragraphs with page numbers.

---

## ğŸ§¾ Deliverable: Ingestion Checklist

When preparing docs for RAG:

1. âœ… Clean the text.
2. âœ… Choose chunk size (500â€“1,200 tokens).
3. âœ… Add overlaps (50â€“200).
4. âœ… Keep splits at natural boundaries.
5. âœ… Attach metadata (source, page, timestamp).
6. âœ… Store in a vector DB (FAISS, Pinecone, Chroma, Weaviate).

---

ğŸ“– Reference:

* [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)
* [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/)

---
## ğŸ”¹ Real Code: Chunking in LangChain

LangChain comes with built-in text splitters. The most common is `RecursiveCharacterTextSplitter`, which tries to split at natural boundaries (paragraphs, then sentences, then words).

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Sample text (pretend this is extracted from a PDF or DOCX)
doc_text = """
Employees are entitled to 18 vacation days per year. 
Unused days cannot be carried over to the next year. 
Sick leave is separate and must be approved by HR. 
Benefits include healthcare coverage, retirement plan options, 
and wellness stipends provided quarterly.
"""

# âŒ BAD CHUNKING: tiny chunks, no overlap
bad_splitter = RecursiveCharacterTextSplitter(
    chunk_size=50,   # way too small
    chunk_overlap=0  # no overlap
)
bad_chunks = bad_splitter.split_text(doc_text)
print("âŒ Bad Chunks:")
for ch in bad_chunks:
    print("-", ch)

# âœ… GOOD CHUNKING: balanced size, overlap included
good_splitter = RecursiveCharacterTextSplitter(
    chunk_size=120,   # larger, keeps context
    chunk_overlap=30  # overlap avoids cutting mid-thought
)
good_chunks = good_splitter.split_text(doc_text)
print("\nâœ… Good Chunks:")
for ch in good_chunks:
    print("-", ch)
```

### ğŸ” What Youâ€™ll See:

* **Bad chunks**: Random sentence fragments like *â€œEmployees are entitled to 18â€¦â€*.
* **Good chunks**: Full, coherent segments like *â€œEmployees are entitled to 18 vacation days per year. Unused days cannot be carriedâ€¦â€*.

---
ğŸ‘‰ Next step would be **Chapter 4: Vector Databases**, where we show *where these chunks live* and *how retrievers pull them back*.

