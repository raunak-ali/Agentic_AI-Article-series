{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X3XonVo_3TfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrbZ379L3jTW"
      },
      "source": [
        "Researcher + Writer Agents (LangGraph) — Step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va66RvR63j9M"
      },
      "source": [
        "Goal: Build a LangGraph workflow where a Researcher agent gathers facts (Wikipedia) and a Writer agent turns those facts into a LinkedIn-style post. State persists research + final post. The agents are created with create_react_agent (LangGraph prebuilt ReAct)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOX3jNIl3qkY"
      },
      "source": [
        "0 — Notes / prerequisites (read before running)\n",
        "\n",
        "You need a Google API key from Google AI Studio (we use Gemini via langchain-google-genai).\n",
        "LangChain\n",
        "\n",
        "This notebook runs in CPU-only mode but works in Colab. If you use Colab, prefer setting the key via getpass or Colab secrets.\n",
        "\n",
        "We use @tool to register callable tools for agents; ensure docstrings + type hints are present.\n",
        "LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK-MmgBP3rQA"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJBKDdhL30Ur"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Users/Raunak Work Profile/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Run this cell first (Colab / Jupyter)\n",
        "!pip install -qU langchain langgraph langchain-google-genai wikipedia pydantic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UezU0mwS32CK"
      },
      "source": [
        "Set your Google API key securely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUB7m_XL35AG",
        "outputId": "2eceaf29-b24b-43e4-c58c-4721290e83f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your Google AI Studio API key (keeps it hidden): ··········\n"
          ]
        }
      ],
      "source": [
        "# In Colab use getpass to avoid leaking your key in logs\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "key = getpass(\"Paste your Google AI Studio API key (keeps it hidden): \")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = key.strip()\n",
        "os.environ[\"GOOGLE_API_KEY\"]=\"AIzaSyBEhOoTh2Iu2UzC1p8Kfz8pL4FxGQP1F_w\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf0qtnid4KZG"
      },
      "source": [
        "Imports & short explanation of core objects.\n",
        "Why these?\n",
        "\n",
        "StateGraph + add_node/add_edge are LangGraph primitives used to model the workflow as a graph.\n",
        "LangChain\n",
        "+1\n",
        "\n",
        "create_react_agent is the recommended prebuilt to create ReAct-style agents in LangGraph.\n",
        "LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZmuM5ywY4Fyy"
      },
      "outputs": [],
      "source": [
        "# Core LLM + LangGraph imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Tools decorator\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Other utilities\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional, Dict, Any\n",
        "import wikipedia, json, re, time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL9qrL1Y4Rtq"
      },
      "source": [
        "Define the typed state model\n",
        "Why typed state?\n",
        "\n",
        "Typed state (Pydantic) gives predictable keys, validation, and easier debugging/visualization in LangGraph Studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1Ni2Nwk34Q-D"
      },
      "outputs": [],
      "source": [
        "# A typed state helps guarantee consistent communication between agents.\n",
        "class MultiAgentState(BaseModel):\n",
        "    topic: str\n",
        "    research_notes: List[str] = []\n",
        "    structured_facts: Dict[str, Any] = {}\n",
        "    draft: Optional[str] = None\n",
        "    final_post: Optional[str] = None\n",
        "    trace: List[Dict[str, Any]] = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsc-D_pO4Zr2"
      },
      "source": [
        "Tool: Wikipedia lookup (register as a tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bA6wA3wV4cMj"
      },
      "outputs": [],
      "source": [
        "# Tools must have a docstring and type annotations. LangChain will use the docstring\n",
        "# as the tool description presented to the LLM.\n",
        "@tool\n",
        "def wiki_lookup(query: str) -> str:\n",
        "    \"\"\"\n",
        "    wiki_lookup(query: str) -> str\n",
        "    Search Wikipedia for the query and return a short JSON string:\n",
        "      {\"title\": \"<page title>\", \"summary\": \"<two-sentence summary>\"}\n",
        "    Returns a JSON string (so the agent can parse it reliably).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        titles = wikipedia.search(query, results=3)\n",
        "        if not titles:\n",
        "            return json.dumps({\"error\": \"no_results\"})\n",
        "        page = wikipedia.page(titles[0], auto_suggest=False)\n",
        "        summary = wikipedia.summary(page.title, sentences=2)\n",
        "        return json.dumps({\"title\": page.title, \"summary\": summary})\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC56cJZu4ee9"
      },
      "source": [
        "Initialize the LLM and create two agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yXtAtmRz4gsg"
      },
      "outputs": [],
      "source": [
        "# Initialize Gemini (use your environment key)\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",  # pick the available model in your account\n",
        "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# Researcher agent: allowed to call wiki_lookup\n",
        "researcher_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[wiki_lookup],\n",
        "    prompt=(\n",
        "        \"You are a Researcher agent. Your job: given a topic, gather up to 5 concise facts. \"\n",
        "        \"When you need factual verification, call the tool `wiki_lookup(query)`. \"\n",
        "        \"Return facts as a JSON array like: \"\n",
        "        '[{\"fact\": \"...\", \"source\":\"<title>\"}].'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Writer agent: no tools, just craft high-quality LinkedIn-style post using research notes.\n",
        "writer_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[],  # writer uses the state.research_notes as input\n",
        "    prompt=(\n",
        "        \"You are a Writer agent. Use the research notes in state.research_notes to write \"\n",
        "        \"a LinkedIn-style post (~150-220 words). Keep it professional and add one sentence \"\n",
        "        \"highlighting the strongest fact from the research (mention source).\"\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYp2k7bJ4iq2"
      },
      "source": [
        "Node functions: how each graph node uses its agent\n",
        "\n",
        "\n",
        "Notes on node behavior:\n",
        "\n",
        "Node functions are intentionally defensive: agents may return free-text; we attempt JSON parse first, then fallback to raw text.\n",
        "\n",
        "Each node returns only the updated fields (LangGraph merges them into the shared state). This aligns with LangGraph’s node semantics.\n",
        "LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wUHHHJk4la_"
      },
      "outputs": [],
      "source": [
        "# Node functions: accept a plain dict `state_dict` and return dict updates (LangGraph convention).\n",
        "\n",
        "def researcher_node(state: MultiAgentState):\n",
        "    \"\"\"\n",
        "    Reads: state.topic\n",
        "    Writes: appends to state_dict['research_notes'], may set structured_facts\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    topic = state.topic\n",
        "    # explicit prompt to make agent return structured JSON\n",
        "    prompt = (\n",
        "        f\"Research the following topic and return up to 15 concise facts as a JSON array.\\n\\n\"\n",
        "        f\"Topic: {topic}\\n\\n\"\n",
        "        \"When needed, call wiki_lookup(query). Each fact object should be: \"\n",
        "        '{\"fact\": \"...\", \"source\":\"<title>\"}'\n",
        "    )\n",
        "\n",
        "    resp = researcher_agent.invoke({\"messages\": [(\"user\", prompt)]})\n",
        "    # robust extraction of generated text from response\n",
        "    content = \"\"\n",
        "    if isinstance(resp, dict):\n",
        "        # many LangGraph prebuilt returns include a messages list\n",
        "        content = resp.get(\"output\") or \"\"\n",
        "        if not content and resp.get(\"messages\"):\n",
        "            content = resp[\"messages\"][-1].content\n",
        "    else:\n",
        "        content = str(resp)\n",
        "\n",
        "    # Try parsing JSON first; if not JSON, keep as single raw note\n",
        "    parsed_facts = []\n",
        "    try:\n",
        "        parsed = json.loads(content)\n",
        "        if isinstance(parsed, list):\n",
        "            for item in parsed:\n",
        "                # normalize into string note\n",
        "                fact = item.get(\"fact\") if isinstance(item, dict) else str(item)\n",
        "                src = item.get(\"source\", \"\")\n",
        "                parsed_facts.append(f\"{fact} (source: {src})\")\n",
        "        elif isinstance(parsed, dict) and parsed.get(\"facts\"):\n",
        "            for item in parsed[\"facts\"]:\n",
        "                parsed_facts.append(item.get(\"fact\",\"\"))\n",
        "    except Exception:\n",
        "        # fallback: put the whole content as one research note\n",
        "        parsed_facts.append(content.strip())\n",
        "\n",
        "    # update state\n",
        "    notes = state.research_notes or []\n",
        "    notes.extend(parsed_facts)\n",
        "    # naive structured extraction: first integer-like number found\n",
        "    first_num = None\n",
        "    m = re.search(r\"(\\d{1,3}(?:[,\\d]{0,})?)\", content.replace(\"\\xa0\",\" \"))\n",
        "    if m:\n",
        "        try:\n",
        "            first_num = int(m.group(1).replace(\",\", \"\"))\n",
        "        except:\n",
        "            first_num = None\n",
        "\n",
        "    structured = state.structured_facts or {}\n",
        "    if first_num is not None and \"first_number\" not in structured:\n",
        "        structured[\"first_number\"] = first_num\n",
        "\n",
        "    trace = state.trace or []\n",
        "    trace.append({\"node\": \"researcher\", \"time\": time.time(), \"raw\": content[:1000]})\n",
        "\n",
        "    return {\"research_notes\": notes, \"structured_facts\": structured, \"trace\": trace}\n",
        "\n",
        "def writer_node(state: MultiAgentState):\n",
        "    \"\"\"\n",
        "    Reads: state.research_notes or []\n",
        "    Writes: sets state_dict['final_post'] (string)\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    notes = state.research_notes or []\n",
        "    prompt = (\n",
        "        \"Using the research notes below, write a LinkedIn-style ~280-word post.\\n\\n\"\n",
        "        \"RESEARCH NOTES:\\n\" + \"\\n\".join(notes) + \"\\n\\n\"\n",
        "        \"Post:\"\n",
        "    )\n",
        "\n",
        "    resp = writer_agent.invoke({\"messages\": [(\"user\", prompt)]})\n",
        "    content = \"\"\n",
        "    if isinstance(resp, dict):\n",
        "        content = resp.get(\"output\") or \"\"\n",
        "        if not content and resp.get(\"messages\"):\n",
        "            content = resp[\"messages\"][-1].content\n",
        "    else:\n",
        "        content = str(resp)\n",
        "\n",
        "    trace = state.trace or []\n",
        "    trace.append({\"node\": \"writer\", \"time\": time.time(), \"raw\": content[:400]})\n",
        "\n",
        "    return {\"final_post\": content.strip(), \"trace\": trace}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_CQnEz-4pc-"
      },
      "source": [
        "Build the graph (nodes + edges) and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCj6SNym4xiU",
        "outputId": "82bc5039-c098-428a-c309-07670b9cd667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph compiled. Ready to run.\n"
          ]
        }
      ],
      "source": [
        "# Create the StateGraph using the Pydantic model as the schema\n",
        "graph = StateGraph(MultiAgentState)\n",
        "\n",
        "# Register nodes\n",
        "graph.add_node(\"researcher\", researcher_node)\n",
        "graph.add_node(\"writer\", writer_node)\n",
        "\n",
        "# Wire the flow: START -> researcher -> writer -> END\n",
        "graph.add_edge(START, \"researcher\")\n",
        "graph.add_edge(\"researcher\", \"writer\")\n",
        "graph.add_edge(\"writer\", END)\n",
        "\n",
        "# Compile to get a runnable app\n",
        "app = graph.compile()\n",
        "print(\"Graph compiled. Ready to run.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMPLFDWQ4zdW"
      },
      "source": [
        "Run the graph on the topic and inspect results:-\n",
        "What to expect:\n",
        "\n",
        "research_notes will contain the Researcher agent’s findings (may be one or multiple items).\n",
        "\n",
        "final_post will be the Writer agent’s LinkedIn-style result.\n",
        "\n",
        "trace shows which nodes ran and a snippet of their raw outputs (useful for debugging)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVEVfwXJ42NR",
        "outputId": "da62ffde-fa50-49aa-bc6a-f8904360ac3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== FINAL POST ===\n",
            "\n",
            "Global food security is facing an unprecedented challenge, and climate change is at the heart of it. Our agricultural systems, vital for feeding a growing population, are increasingly strained, making it harder to provide a stable food supply worldwide.\n",
            "\n",
            "The impacts are clear and multifaceted. We're seeing how rising global temperatures and unpredictable weather patterns are directly contributing to lower crop yields across various regions. This isn't just about heat; water scarcity, driven by more frequent droughts, intense heat waves, and even devastating floods, further exacerbates the problem, making it harder for essential crops to thrive and reach their full potential.\n",
            "\n",
            "It's a stark reality: according to \"Effects of climate change on agriculture,\" rising temperatures and changing weather patterns often result in lower crop yields. This critical insight underscores the direct link between our changing climate and the very foundation of our food supply.\n",
            "\n",
            "Ensuring a stable and secure food supply for everyone requires urgent attention to these climate-driven agricultural challenges. Building resilient food systems, adapting farming practices, and investing in sustainable solutions are no longer options, but necessities. What innovative approaches do you believe are most crucial for safeguarding our food future?\n",
            "\n",
            "=== RESEARCH NOTES ===\n",
            "1. ```json\n",
            "[\n",
            "  {\n",
            "    \"fact\": \"Climate change makes it harder for agricultural activities to provide global food security.\",\n",
            "    \"source\": \"Effects of climate change on agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"fact\": \"Rising temperatures and changing weather patterns often result in lower crop yields.\",\n",
            "    \"source\": \"Effects of climate change on agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"fact\": \"Water scarcity caused by drought, heat waves, and flooding contributes to lower crop yields.\",\n",
            "    \"source\": \"Effects of climate change on agriculture\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "=== TRACE (short) ===\n",
            "researcher - ```json\n",
            "[\n",
            "  {\n",
            "    \"fact\": \"Climate change makes it harder for agricultural activities to provide global food security.\",\n",
            "writer - Global food security is facing an unprecedented challenge, and climate change is at the heart of it. Our agricultural sy\n"
          ]
        }
      ],
      "source": [
        "# Provide just the topic; the graph/state model defaults the rest\n",
        "input_state = {\"topic\": \"How Agentic Ai is better than Gen Ai\"}\n",
        "\n",
        "# Run the compiled graph app\n",
        "result = app.invoke(input_state)\n",
        "\n",
        "# Inspect final output and trace\n",
        "print(\"=== FINAL POST ===\\n\")\n",
        "print(result.get(\"final_post\") or result.get(\"draft\") or \"No final post produced.\\n\")\n",
        "\n",
        "print(\"\\n=== RESEARCH NOTES ===\")\n",
        "for idx, n in enumerate(result.get(\"research_notes\", []), 1):\n",
        "    print(f\"{idx}. {n}\")\n",
        "\n",
        "print(\"\\n=== TRACE (short) ===\")\n",
        "for t in result.get(\"trace\", []):\n",
        "    print(t[\"node\"], \"-\", t.get(\"raw\", \"\")[:120])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvWMNcaX47_f"
      },
      "source": [
        "(Optional) Visualize the graph in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bmeAQuFn5ANn",
        "outputId": "2ace8ce8-5df8-4ca8-d653-7330ce0cbaa7"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```mermaid\n",
              "---\n",
              "config:\n",
              "  flowchart:\n",
              "    curve: linear\n",
              "---\n",
              "graph TD;\n",
              "\t__start__([<p>__start__</p>]):::first\n",
              "\tresearcher(researcher)\n",
              "\twriter(writer)\n",
              "\t__end__([<p>__end__</p>]):::last\n",
              "\t__start__ --> researcher;\n",
              "\tresearcher --> writer;\n",
              "\twriter --> __end__;\n",
              "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
              "\tclassDef first fill-opacity:0\n",
              "\tclassDef last fill:#bfb6fc\n",
              "\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the compiled graph as mermaid; works in many notebook UIs\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "try:\n",
        "    mermaid = app.get_graph().draw_mermaid()\n",
        "    display(Markdown(\"```mermaid\\n\" + mermaid + \"\\n```\"))\n",
        "except Exception as e:\n",
        "    print(\"Graph drawing may not be supported in this environment:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13OzVNJb5EyO"
      },
      "source": [
        "# Troubleshooting & Tips (brief but critical)\n",
        "\n",
        "If you see InvalidArgument: contents is not specified, ensure you call agents with a proper messages payload (example above uses {\"messages\":[(\"user\", prompt)]}). If your environment requires role-content dicts, try {\"messages\":[{\"role\":\"user\",\"content\":prompt}]}. (Different wrappers accept slightly different message formats.)\n",
        "\n",
        "If agents return large free text instead of JSON: tighten the state_modifier and prompt to require JSON-only output with an exact schema example.\n",
        "\n",
        "For production, add timeouts, max steps, and step limits to avoid runaway loops. Cache frequent wiki queries to reduce latency/cost.\n",
        "\n",
        "If Wikipedia misses facts, consider SerpAPI / official data sources for higher reliability (but expect API keys/costs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0IkXZIrA2e2"
      },
      "source": [
        "# Next steps you can try : ---\n",
        "(A) add a small \"Supervisor\" node (optional) that checks research quality and re-runs researcher if notes are too short, or\n",
        "\n",
        "(B) add a parallel-researchers + aggregator variant (more advanced)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWHINUFdA70y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
